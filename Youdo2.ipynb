{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e07ddfab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-69-07808ea6c93e>:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\", delimiter = r'\\t',\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.,  3.,  4., ..., nan, nan, nan],\n",
       "       [ 4., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [ 5., nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan,  5., nan, ..., nan, nan, nan]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\", delimiter = r'\\t', \n",
    "                 names = ['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "r = df.pivot(index = 'user_id', columns = 'item_id', values = 'rating').values\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5ce31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "irow, jcol = np.where(~np.isnan(r)) # Burası matriste değere sahip olan entrylerin row ve column indexlerini veriyor.\n",
    "# 100_000 dolu entry mevcut\n",
    "\n",
    "idx = np.random.choice(np.arange(100_000), 1000, replace=False) # 1 ile 100k arası 1000 adet tekrarı olmadan random sayı seçiyor\n",
    "test_irow = irow[idx] # 100_000 entry içerisinden random seçilen idx listesindeki indexlere sahip entryler test row ve col \n",
    "test_jcol = jcol[idx] # olarak seçilir.\n",
    "\n",
    "r_copy = r.copy() # r bizim orijinal matrisimiz, r_copy ise test matrisimiz (tahmini gerçekleştirdiğimiz) olacak.\n",
    "\n",
    "for i, j in zip(test_irow, test_jcol):\n",
    "    r_copy[i][j] = np.nan\n",
    "    \n",
    "\n",
    "#r[test_irow, test_jcol], r_copy[test_irow, test_jcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d144521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation split (Masking validation entries)\n",
    "\n",
    "# Artık r_copy üzerinden çalışacağım için validation set'i de buradan ayıracağım\n",
    "irow2, jcol2 = np.where(~np.isnan(r_copy))\n",
    "\n",
    "idx2 = np.random.choice(np.arange(99_000), 300, replace=False)\n",
    "\n",
    "test_irow2 = irow2[idx2]\n",
    "test_jcol2 = jcol2[idx2]\n",
    "\n",
    "r_copy2 = r_copy.copy() # r_copy2 hem test hem validation set'in maskelenmiş hali (öğrendiğimiz model) olacak.\n",
    "\n",
    "for i, j in zip(test_irow2, test_jcol2):\n",
    "    r_copy2[i][j] = np.nan\n",
    "        \n",
    "# Buranın sonunda artık b_user ve b_item optimizasyonunu sağlayacak dataları belirlemiş oluyorum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "230651c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Validation Split (Masking train entries)\n",
    "\n",
    "A = np.arange(99_000)\n",
    "idx3 = np.array(list(set(A) - set(idx2))) # Burada idx2 setinde olan indisleri A'dan çıkarıyoruz.\n",
    "\n",
    "r_copy3 = r_copy.copy() # r bizim orijinal matrisimiz, r_copy ise test matrisimiz (tahmini gerçekleştirdiğimiz) olacak.\n",
    "\n",
    "test_irow3 = irow2[idx3]\n",
    "test_jcol3 = jcol2[idx3]\n",
    "\n",
    "for i, j in zip(test_irow3, test_jcol3):\n",
    "    r_copy3[i][j] = np.nan\n",
    "\n",
    "#r_copy3[test_irow3, test_jcol3], r_copy[test_irow3, test_jcol3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "85458dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and gradient descent without regularization.\n",
    "\n",
    "def gradientDescent(r: np.ndarray, b_user: np.ndarray, b_item: np.ndarray, alpha: float = 0.001, \n",
    "                    ite: int = 1000) -> np.ndarray:\n",
    "    \n",
    "    error_in_each_ite = []\n",
    "    m, n = r.shape\n",
    "\n",
    "    for it in range(ite):\n",
    "        total_e = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if np.isnan(r[i, j]):\n",
    "                    continue        \n",
    "                else:\n",
    "                    # Prediction of r_ij\n",
    "                    y_pred = b_user[i][0] + b_item[j][0]\n",
    "\n",
    "                    e = r[i][j] - y_pred\n",
    "                    \n",
    "                    Loss = (((r[i][j] - y_pred) ** 2) / 2)\n",
    "\n",
    "                    b_user[i][0] += e * alpha \n",
    "                    b_item[j][0] += e * alpha\n",
    "\n",
    "                    total_e += e\n",
    "        \n",
    "        print(it, total_e)\n",
    "        error_in_each_ite.append(total_e) \n",
    "\n",
    "        if it > 2:\n",
    "            if (error_in_each_ite[it - 2] - error_in_each_ite[it - 1] < 0.001):\n",
    "                break \n",
    "    \n",
    "    return b_user, b_item, error_in_each_ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3c479171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction and gradient descent with l2 regularization.\n",
    "\n",
    "def l2regularized(r: np.ndarray, b_user: np.ndarray, b_item: np.ndarray, lamb: float = 0., \n",
    "                  alpha: float = 0.001, ite: int = 1000) -> np.ndarray:\n",
    "    error_in_each_ite = []\n",
    "    m, n = r.shape\n",
    "    for it in range(ite):\n",
    "        total_e = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if np.isnan(r[i, j]):\n",
    "                    continue        \n",
    "                else:\n",
    "                    # Prediction of r_ij\n",
    "                    y_pred = b_user[i][0] + b_item[j][0]\n",
    "\n",
    "                    Loss = (((r[i][j] - y_pred) ** 2) / 2) + (lamb / 2 * (b_user[i][0] ** 2 + b_item[j][0] ** 2))\n",
    "                    \n",
    "                    e = r[i][j] - y_pred\n",
    "\n",
    "                    b_user[i][0] += (e - lamb * (b_user[i][0])) * alpha\n",
    "                    b_item[j][0] += (e - lamb * (b_item[j][0])) * alpha\n",
    "\n",
    "                    total_e += e\n",
    "                    \n",
    "        print(it, total_e)\n",
    "        error_in_each_ite.append(total_e) \n",
    "\n",
    "        if it > 2:\n",
    "            if (error_in_each_ite[it - 2] - error_in_each_ite[it - 1] < 0.001):\n",
    "                break \n",
    "    \n",
    "    return b_user, b_item, error_in_each_ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1cebddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 208704.6769963375\n",
      "1 147865.73080480893\n",
      "2 107586.97891570322\n",
      "3 80502.4124296429\n",
      "4 61979.04944677193\n",
      "5 49076.112056158345\n",
      "6 39909.31792571837\n",
      "7 33259.503734732185\n",
      "8 28329.774007161013\n",
      "9 24593.632591799596\n",
      "10 21699.268816489835\n",
      "11 19408.760243816952\n",
      "12 17559.146256100496\n",
      "13 16037.301522121077\n",
      "14 14763.580969228646\n",
      "15 13681.081648978652\n",
      "16 12748.52869298426\n",
      "17 11935.517740970276\n",
      "18 11219.301865808508\n",
      "19 10582.599187539045\n",
      "20 10012.080815240022\n",
      "21 9497.3163103629\n",
      "22 9030.029700082385\n",
      "23 8603.568324212623\n",
      "24 8212.519013551191\n",
      "25 7852.4273176222405\n",
      "26 7519.5895809279245\n",
      "27 7210.897081903717\n",
      "28 6923.7177935525115\n",
      "29 6655.805635255135\n",
      "30 6405.230038366219\n",
      "31 6170.32068890725\n",
      "32 5949.623733428102\n",
      "33 5741.86673511083\n",
      "34 5545.930378004149\n",
      "35 5360.825426807559\n",
      "36 5185.673818332554\n",
      "37 5019.693030139901\n",
      "38 4862.183070496544\n",
      "39 4712.515581687422\n",
      "40 4570.124659815728\n",
      "41 4434.499078453009\n",
      "42 4305.175667911821\n",
      "43 4181.733651581329\n",
      "44 4063.7897793851494\n",
      "45 3950.9941286739886\n",
      "46 3843.02646674457\n",
      "47 3739.593088157115\n",
      "48 3640.424055204908\n",
      "49 3545.2707821175027\n",
      "50 3453.9039134779596\n",
      "51 3366.111455399492\n",
      "52 3281.6971246058934\n",
      "53 3200.478885993375\n",
      "54 3122.2876537394677\n",
      "55 3046.9661347542237\n",
      "56 2974.3677963743958\n",
      "57 2904.3559428052445\n",
      "58 2836.802886999396\n",
      "59 2771.5892065057483\n",
      "60 2708.6030733811785\n",
      "61 2647.7396495814874\n",
      "62 2588.9005403756014\n",
      "63 2531.9932992897698\n",
      "64 2476.93097891254\n",
      "65 2423.6317226005335\n",
      "66 2372.018392732857\n",
      "67 2322.018231690705\n",
      "68 2273.562552191912\n",
      "69 2226.5864540066423\n",
      "70 2181.0285644230944\n",
      "71 2136.830800131492\n",
      "72 2093.938148456402\n",
      "73 2052.2984660944608\n",
      "74 2011.862293716863\n",
      "75 1972.5826849702514\n",
      "76 1934.4150485661185\n",
      "77 1897.3170022837912\n",
      "78 1861.248237833735\n",
      "79 1826.170395634609\n",
      "80 1792.0469486518027\n",
      "81 1758.843094528796\n",
      "82 1726.525655318742\n",
      "83 1695.0629841883695\n",
      "84 1664.424878527089\n",
      "85 1634.582498946953\n",
      "86 1605.5082937055856\n",
      "87 1577.175928128413\n",
      "88 1549.5602186430608\n",
      "89 1522.6370710740675\n",
      "90 1496.3834228768505\n",
      "91 1470.7771890168158\n",
      "92 1445.7972112262491\n",
      "93 1421.4232103921852\n",
      "94 1397.6357418510947\n",
      "95 1374.41615338304\n",
      "96 1351.746545715908\n",
      "97 1329.6097353652185\n",
      "98 1307.9892196488618\n",
      "99 1286.8691437287541\n"
     ]
    }
   ],
   "source": [
    "# Initializing the random bias values for each user and item\n",
    "b_user = np.random.rand(r.shape[0], 1)\n",
    "b_item = np.random.rand(r.shape[1], 1)\n",
    "\n",
    "b_user_gd, b_item_gd, errors_gd = gradientDescent(r_copy2, b_user, b_item, ite = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8d0a3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 210931.78727734098\n",
      "1 151992.5833387941\n",
      "2 113994.85527031099\n",
      "3 89010.25999244789\n",
      "4 72229.73992335981\n",
      "5 60700.15972931502\n",
      "6 52585.360568000004\n",
      "7 46729.06489194957\n",
      "8 42393.50087940504\n",
      "9 39101.43720495006\n",
      "10 36539.713857791816\n",
      "11 34499.715283283025\n",
      "12 32840.25571583852\n",
      "13 31464.194014172714\n",
      "14 30303.54033167515\n",
      "15 29309.865871227685\n",
      "16 28448.05671368381\n",
      "17 27692.197337858066\n",
      "18 27022.824245036685\n",
      "19 26425.070247239386\n",
      "20 25887.39400663335\n",
      "21 25400.698437257968\n",
      "22 24957.71046385328\n",
      "23 24552.53853151426\n",
      "24 24180.352482872582\n",
      "25 23837.1487279506\n",
      "26 23519.575618364415\n",
      "27 23224.801859661045\n",
      "28 22950.416082491192\n",
      "29 22694.34925688594\n",
      "30 22454.814060520057\n",
      "31 22230.25698149874\n",
      "32 22019.32009727277\n",
      "33 21820.810287337194\n",
      "34 21633.67421715356\n",
      "35 21456.97784709433\n",
      "36 21289.889522410434\n",
      "37 21131.665921901502\n",
      "38 20981.640307243986\n",
      "39 20839.212637899043\n",
      "40 20703.841209485072\n",
      "41 20575.035544378687\n",
      "42 20452.350317904275\n",
      "43 20335.380145826966\n",
      "44 20223.75509200278\n",
      "45 20117.136781173172\n",
      "46 20015.21502265073\n",
      "47 19917.704867190987\n",
      "48 19824.344032737103\n",
      "49 19734.89064545847\n",
      "50 19649.121251336153\n",
      "51 19566.829060686345\n",
      "52 19487.82239395488\n",
      "53 19411.923301948897\n",
      "54 19338.966337748534\n",
      "55 19268.797460882306\n",
      "56 19201.2730571709\n",
      "57 19136.259060006978\n",
      "58 19073.630160821012\n",
      "59 19013.269098166656\n",
      "60 18955.06601627149\n",
      "61 18898.917885133047\n",
      "62 18844.727975235288\n",
      "63 18792.405380884924\n",
      "64 18741.864586883326\n",
      "65 18693.025073935692\n",
      "66 18645.81095873346\n",
      "67 18600.150665143938\n",
      "68 18555.976623362552\n",
      "69 18513.22499423597\n",
      "70 18471.835416298538\n",
      "71 18431.75077332268\n",
      "72 18392.916980439968\n",
      "73 18355.282787102245\n",
      "74 18318.799595320754\n",
      "75 18283.421291804072\n",
      "76 18249.104092760444\n",
      "77 18215.80640022989\n",
      "78 18183.488668964626\n",
      "79 18152.113282943406\n",
      "80 18121.64444071007\n",
      "81 18092.048048800596\n",
      "82 18063.29162259452\n",
      "83 18035.344193989175\n",
      "84 18008.176225348714\n",
      "85 17981.75952923426\n",
      "86 17956.067193462608\n",
      "87 17931.07351108319\n",
      "88 17906.75391489927\n",
      "89 17883.08491618935\n",
      "90 17860.044047319727\n",
      "91 17837.609807960365\n",
      "92 17815.761614642073\n",
      "93 17794.47975341218\n",
      "94 17773.745335378106\n",
      "95 17753.540254921147\n",
      "96 17733.847150409194\n",
      "97 17714.649367224847\n",
      "98 17695.9309229579\n",
      "99 17677.676474617187\n"
     ]
    }
   ],
   "source": [
    "# Initializing the random bias values for each user and item\n",
    "b_user = np.random.rand(r.shape[0], 1)\n",
    "b_item = np.random.rand(r.shape[1], 1)\n",
    "\n",
    "b_user_l2, b_item_l2, errors_l2 = l2regularized(r_copy2, b_user, b_item, lamb = 0.1, ite = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "43b23f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(r: np.ndarray, b_user: np.ndarray, b_item: np.ndarray):\n",
    "    m, n = r.shape\n",
    "    r_hat = np.zeros((m, n))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            y_pred = b_user[i][0] + b_item[j][0]\n",
    "            r_hat[i][j] = y_pred\n",
    "            \n",
    "    return r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "62f44ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hat_gd = pred(r_copy2, b_user_gd, b_item_gd)\n",
    "r_hat_l2 = pred(r_copy2, b_user_l2, b_item_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "303103ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(893.2067042236139, 925.802434710798)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum of squares errors without and with regularization, respectively. Before hyperparameter optimization.\n",
    "sq_error_gd = np.nansum((r[test_irow, test_jcol] - r_hat_gd[test_irow, test_jcol]) ** 2) \n",
    "sq_error_l2 = np.nansum((r[test_irow, test_jcol] - r_hat_l2[test_irow, test_jcol]) ** 2) \n",
    "\n",
    "sq_error_gd, sq_error_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35919785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss fonksiyonu lambda'ya göre convex olduğu için aynı şekilde iterasyonla hyperparameter optimization yapacağım.\n",
    "\n",
    "def lam_opt(r: np.ndarray, b_user: np.ndarray, b_item: np.ndarray, alpha: float = 0.001, ite: int = 1000) -> np.ndarray:\n",
    "    \n",
    "    lambd = (np.random.rand(1)*100)[0]\n",
    "    error_in_each_ite = []\n",
    "    m, n = r.shape\n",
    "    for it in range(ite):\n",
    "        total_e = 0\n",
    "\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                if np.isnan(r[i, j]):\n",
    "                    continue        \n",
    "                else:\n",
    "                    # Prediction of r_ij\n",
    "                    y_pred = b_user[i][0] + b_item[j][0]\n",
    "                    \n",
    "                    e = r[i][j] - y_pred\n",
    "                    \n",
    "                    Loss = ((e ** 2) / 2) + (lambd / 2 * (b_user[i][0] ** 2 + b_item[j][0] ** 2))\n",
    "\n",
    "                    lambd -= ((b_user[i][0] ** 2 + b_item[j][0] ** 2) / 2) * alpha\n",
    "                    \n",
    "                    total_e += e\n",
    "                    \n",
    "        print(it, total_e)\n",
    "        error_in_each_ite.append(total_e) \n",
    "\n",
    "        if it > 2:\n",
    "            if (error_in_each_ite[it - 2] - error_in_each_ite[it - 1] < 0.001):\n",
    "                break \n",
    "    \n",
    "    return lambd, error_in_each_ite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e053fa9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -10.57715738197612\n",
      "1 -10.57715738197612\n",
      "2 -10.57715738197612\n",
      "3 -10.57715738197612\n"
     ]
    }
   ],
   "source": [
    "optimal_lamb, lam_errors_gd = lam_opt(r_copy3, b_user_l2, b_item_l2, ite = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization için yine convexity kullanmak aklıma geldi fakat ben lambd değiştirdiğimde ve artık b_user \n",
    "# ve b_item'ı sabit tuttuğumda bunun y_pred'e bir etkisi olmuyor. Farklı bir yöntem olarak yalnızca lambda değerlerini \n",
    "# değiştirerek en düşük loss'u verecek olan lambdayı seçebilirdim fakat bu da nedense kullanabileceğim en doğr yöntemmiş\n",
    "# gibi gelmiyor. Buradan nasıl devam edebileceğim konusunda bir feedback verebilir misiniz? \n",
    "\n",
    "# Artık buradan devam etmeyeceğim çünkü benzer bir problemle capstone projede de karşılaşacağım, en azından vaktimi o tarafa\n",
    "# harcamak daha mantıklı geliyor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
